{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df= pd.read_csv('permission_malware.csv')\n",
    "benign_df= pd.read_csv('permission_benign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df['Label']=[1]*len(malicious_df)\n",
    "benign_df['Label']=[0]*len(benign_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([malicious_df, benign_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df.columns[[0]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = X, y, X, y #train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "classifier = RandomForestClassifier(n_estimators =125, n_jobs=-1, random_state=0 )\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with random forests: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with random forests: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "RF =RandomForestRegressor(n_estimators =125, n_jobs=-1, random_state=0 )\n",
    "RF.fit(X, y_probs)\n",
    "y_pred = RF.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with Decision Trees: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with Decision Trees: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "DT =DecisionTreeRegressor(random_state=0)\n",
    "DT.fit(X, y_probs)\n",
    "y_pred = DT.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier=AdaBoostClassifier()\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with AdaBoost: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with AdaBoost: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "AdaBoost =AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "AdaBoost= MultiOutputRegressor(AdaBoost)\n",
    "AdaBoost.fit(X, y_probs)\n",
    "y_pred = AdaBoost.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', probability=True)\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with SVC: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with SVC: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "SVR =SVR(kernel = 'rbf')\n",
    "SVR= MultiOutputRegressor(SVR)\n",
    "SVR.fit(X, y_probs)\n",
    "y_pred = SVR.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier =GradientBoostingClassifier()\n",
    "classifier.fit(X, y)\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with SVC: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with SVC: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "GradientBoosting =GradientBoostingRegressor(random_state=0)\n",
    "GradientBoosting= MultiOutputRegressor(GradientBoosting)\n",
    "GradientBoosting.fit(X, y_probs)\n",
    "y_pred = GradientBoosting.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "classifier = ExtraTreesClassifier(n_jobs=-1)\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with SVC: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with SVC: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "ExtraTree =ExtraTreesRegressor(random_state=0 ,n_jobs=-1)\n",
    "ExtraTree.fit(X, y_probs)\n",
    "y_pred =ExtraTree.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "classifier = BaggingClassifier(base_estimator=SVC(probability=True), n_jobs=-1)\n",
    "classifier.fit(X, y.reshape(-1))\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with SVC: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with SVC: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "Bagging=BaggingRegressor(base_estimator=SVR(),n_jobs=-1, random_state=0)\n",
    "Bagging= MultiOutputRegressor(Bagging)\n",
    "Bagging.fit(X, y_probs)\n",
    "y_pred = Bagging.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier part\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X, y)\n",
    "y_pred = classifier.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "y_probs = classifier.predict_proba(X)\n",
    "print(\"Confusion matrix with SVC: \")\n",
    "print(cm)\n",
    "print(\"Accuracy with SVC: \")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(y_probs[:10])\n",
    "\n",
    "#Regressor part\n",
    "from xgboost import  XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "XGB =XGBRegressor()\n",
    "XGB= MultiOutputRegressor(XGB)\n",
    "XGB.fit(X, y_probs)\n",
    "y_pred = XGB.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix with Distillation: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasetcustom(Dataset):\n",
    "    \n",
    "    def __init__(self,data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels=labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        image = torch.from_numpy(self.data[index]).float()\n",
    "        label = torch.tensor(self.labels[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers={ \"AdaBoost\": AdaBoost, \"DecisionTrees\": DT, \"SupportVectorClassifier\": SVR , \"Random_Forests\": RF }\n",
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Algorithm in Classifiers:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(1,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Classifiers[Algorithm]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "        target_model= Classifiers[Algorithm]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = classifier.predict(images.detach().numpy().reshape(1,-1))\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1:\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = classifier.predict(images.detach().numpy().reshape(1,-1))\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j,3]+=1\n",
    "                            df1.iloc[j,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,10):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,10):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers={ \"Gradient_Boosting_Classifier\": GradientBoosting, \"XGBoost_classifier\": XGB, \"Extra_Tree_classifier\": ExtraTree , \n",
    "             \"Bagging_Classifier\":Bagging }\n",
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Algorithm in Classifiers:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(1,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Classifiers[Algorithm]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "        target_model= Classifiers[Algorithm]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = classifier.predict(images.detach().numpy().reshape(1,-1))\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1:\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = classifier.predict(images.detach().numpy().reshape(1,-1))\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j,3]+=1\n",
    "                            df1.iloc[j,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,10):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,10):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, y_probs, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasetcustom(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset= Datasetcustom(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_dataset=Datasetcustom(malicious_df.iloc[:,1:-1].values, malicious_df.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lab = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img)\n",
    "print(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "mal_loader=DataLoader(mal_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_iter.next()\n",
    "print(images.dtype)\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(195, 512),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(512, 256),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(256, 64),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(64, 2),\n",
    "#                       nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 512)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(512, 256)\n",
    "        self.l3=nn.Linear(256, 64)\n",
    "        self.l4=nn.Linear(64, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l4(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=150\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "train_iter = iter(probs_loader)\n",
    "images, labels = train_iter.next()\n",
    "y_probs= model(images)\n",
    "y_probs=y_probs.detach().numpy()\n",
    "X_probs=images.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_probs=y_probs*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_probs, y_probs, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dist = Datasetcustom(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dist= Datasetcustom(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_dist = DataLoader(train_dataset_dist, batch_size=20, shuffle=True)\n",
    "test_loader_dist = DataLoader(test_dataset_dist, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_dist(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_dist, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 512)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(512, 256)\n",
    "        self.l3=nn.Linear(256, 64)\n",
    "        self.l4=nn.Linear(64, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l4(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = nn.Sequential(nn.Linear(195, 512),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(512, 256),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(256, 64),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.Linear(64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=model_dist(5)\n",
    "criterion_dist =  nn.MSELoss()\n",
    "optimizer_dist = optim.SGD(model1.parameters(), lr=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_dist = iter(train_loader_dist)\n",
    "print(type(train_iter_dist))\n",
    "\n",
    "images, labels = train_iter_dist.next()\n",
    "print(images.dtype)\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=40\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader_dist:\n",
    "        \n",
    "        optimizer_dist.zero_grad()\n",
    "#         print(labels)\n",
    "        log_ps = model1(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion_dist(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer_dist.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "#         accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader_dist:\n",
    "                log_ps = model1(images)\n",
    "                test_loss += criterion_dist(log_ps, labels)\n",
    "                \n",
    "#                 top_p, top_class = log_ps.topk(1, dim=1)\n",
    "#                 equals = top_class == labels.view(*top_class.shape)\n",
    "#                 accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader_dist))\n",
    "        test_losses.append(test_loss/len(test_loader_dist))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader_dist)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('Train-512-256-64.pth')\n",
    "Models={ \"4-layered\": (model1,5620) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack with original attacker after retraining\n",
    "for Algorithm in Models:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(0,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Models[Algorithm][0]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "#         df=pd.read_csv(Algorithm+'_distillation.csv')\n",
    "#         X=df.iloc[:,:-1].values\n",
    "#         y=df.iloc[:,-1].values\n",
    "        target_model= Models[Algorithm][0]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = target_model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1: #predicted malicious\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = target_model(images)\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j+1,3]+=1\n",
    "                            df1.iloc[j+1,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  For 2 Layered Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasetcustom(X, y)\n",
    "test_dataset = Datasetcustom(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)\n",
    "mal_loader=DataLoader(mal_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_2, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2= model_2(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=100\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model_2(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                log_ps = model_2(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "train_iter = iter(probs_loader)\n",
    "images, labels = train_iter.next()\n",
    "y_probs= model_2(images)\n",
    "y_probs=y_probs.detach().numpy()\n",
    "X_probs=images.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_probs, y_probs, test_size = 0.2, random_state = 0)\n",
    "train_dataset_dist = Datasetcustom(X_train, y_train)\n",
    "test_dataset_dist= Datasetcustom(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_dist = DataLoader(train_dataset_dist, batch_size=20, shuffle=True)\n",
    "test_loader_dist = DataLoader(test_dataset_dist, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2dist(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_2dist, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2dist=model_2dist(5)\n",
    "criterion_dist =  nn.MSELoss()\n",
    "optimizer_dist = optim.SGD(model_2dist.parameters(), lr=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_dist = iter(train_loader_dist)\n",
    "print(type(train_iter_dist))\n",
    "\n",
    "images, labels = train_iter_dist.next()\n",
    "print(images.dtype)\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=40\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader_dist:\n",
    "        \n",
    "        optimizer_dist.zero_grad()\n",
    "#         print(labels)\n",
    "        log_ps = model_2dist(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion_dist(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer_dist.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader_dist:\n",
    "                log_ps = model_2dist(images)\n",
    "                test_loss += criterion_dist(log_ps, labels)   \n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader_dist))\n",
    "        test_losses.append(test_loss/len(test_loader_dist))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader_dist)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('Train-512-256-64.pth')\n",
    "Models={ \"2-layered\": (model_2dist,5620) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack with original attacker after retraining\n",
    "for Algorithm in Models:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(0,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Models[Algorithm][0]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "#         df=pd.read_csv(Algorithm+'_distillation.csv')\n",
    "#         X=df.iloc[:,:-1].values\n",
    "#         y=df.iloc[:,-1].values\n",
    "        target_model= Models[Algorithm][0]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = target_model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1: #predicted malicious\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = target_model(images)\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j+1,3]+=1\n",
    "                            df1.iloc[j+1,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for 3 Layered Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasetcustom(X, y)\n",
    "test_dataset = Datasetcustom(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)\n",
    "mal_loader=DataLoader(mal_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_3(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_3, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 200)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(200, 200)\n",
    "        self.l3=nn.Linear(200, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3= model_3(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_3.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=100\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model_3(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                log_ps = model_3(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "train_iter = iter(probs_loader)\n",
    "images, labels = train_iter.next()\n",
    "y_probs= model_3(images)\n",
    "y_probs=y_probs.detach().numpy()\n",
    "X_probs=images.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_probs, y_probs, test_size = 0.2, random_state = 0)\n",
    "train_dataset_dist = Datasetcustom(X_train, y_train)\n",
    "test_dataset_dist= Datasetcustom(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_dist = DataLoader(train_dataset_dist, batch_size=20, shuffle=True)\n",
    "test_loader_dist = DataLoader(test_dataset_dist, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_3dist(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_3dist, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 200)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(200, 200)\n",
    "        self.l3=nn.Linear(200, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3dist=model_3dist(5)\n",
    "criterion_dist =  nn.MSELoss()\n",
    "optimizer_dist = optim.SGD(model_3dist.parameters(), lr=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_dist = iter(train_loader_dist)\n",
    "print(type(train_iter_dist))\n",
    "\n",
    "images, labels = train_iter_dist.next()\n",
    "print(images.dtype)\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=40\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader_dist:\n",
    "        \n",
    "        optimizer_dist.zero_grad()\n",
    "#         print(labels)\n",
    "        log_ps = model_3dist(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion_dist(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer_dist.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader_dist:\n",
    "                log_ps = model_3dist(images)\n",
    "                test_loss += criterion_dist(log_ps, labels)   \n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader_dist))\n",
    "        test_losses.append(test_loss/len(test_loader_dist))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader_dist)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('Train-512-256-64.pth')\n",
    "Models={ \"3-layered\": (model_3dist,5620) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack with original attacker after retraining\n",
    "for Algorithm in Models:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(0,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Models[Algorithm][0]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "#         df=pd.read_csv(Algorithm+'_distillation.csv')\n",
    "#         X=df.iloc[:,:-1].values\n",
    "#         y=df.iloc[:,-1].values\n",
    "        target_model= Models[Algorithm][0]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = target_model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1: #predicted malicious\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = target_model(images)\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j+1,3]+=1\n",
    "                            df1.iloc[j+1,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for 5  Layered Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Datasetcustom(X, y)\n",
    "test_dataset = Datasetcustom(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)\n",
    "mal_loader=DataLoader(mal_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_5(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_5, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 512)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(512, 256)\n",
    "        self.l3= nn.Linear(256, 64)\n",
    "        self.l4=nn.Linear(64, 32)\n",
    "        self.l5=nn.Linear(32, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l4(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l5(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5= model_5(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_5.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=100\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model_5(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                log_ps = model_5(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "train_iter = iter(probs_loader)\n",
    "images, labels = train_iter.next()\n",
    "y_probs= model_5(images)\n",
    "y_probs=y_probs.detach().numpy()\n",
    "X_probs=images.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_probs, y_probs, test_size = 0.2, random_state = 0)\n",
    "train_dataset_dist = Datasetcustom(X_train, y_train)\n",
    "test_dataset_dist= Datasetcustom(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_dist = DataLoader(train_dataset_dist, batch_size=20, shuffle=True)\n",
    "test_loader_dist = DataLoader(test_dataset_dist, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_5dist(nn.Module):\n",
    "    def __init__(self,t):\n",
    "        super(model_5dist, self).__init__()\n",
    "        self.t=t\n",
    "        self.l1=nn.Linear(195, 512)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.l2=nn.Linear(512, 256)\n",
    "        self.l3= nn.Linear(256, 64)\n",
    "        self.l4=nn.Linear(64, 32)\n",
    "        self.l5=nn.Linear(32, 2)\n",
    "        self.dropout= nn.Dropout(0.4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.l1(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l2(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l3(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l4(x)\n",
    "        x=self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.l5(x)\n",
    "        x= nn.Softmax(dim=1)(x/self.t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5dist=model_5dist(5)\n",
    "criterion_dist =  nn.MSELoss()\n",
    "optimizer_dist = optim.SGD(model_5dist.parameters(), lr=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_dist = iter(train_loader_dist)\n",
    "print(type(train_iter_dist))\n",
    "\n",
    "images, labels = train_iter_dist.next()\n",
    "print(images.dtype)\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 40\n",
    "steps = 0\n",
    "epochs=40\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader_dist:\n",
    "        \n",
    "        optimizer_dist.zero_grad()\n",
    "#         print(labels)\n",
    "        log_ps = model_5dist(images)\n",
    "#         print(log_ps)\n",
    "        loss = criterion_dist(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer_dist.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader_dist:\n",
    "                log_ps = model_5dist(images)\n",
    "                test_loss += criterion_dist(log_ps, labels)   \n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader_dist))\n",
    "        test_losses.append(test_loss/len(test_loader_dist))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader_dist)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('Train-512-256-64.pth')\n",
    "Models={ \"5-layered\": (model_5dist,5620) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('Train-512-256-64.pth')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack with original attacker after retraining\n",
    "for Algorithm in Models:\n",
    "        df1=pd.DataFrame(columns = ['bits_flipped','TP', 'TN','FN'])\n",
    "        for i in range(0,11):\n",
    "            df1=df1.append(pd.Series([i, 5553, 0, 0], index= df1.columns), ignore_index=True )\n",
    "        print(\"Classifier used here is : {}\".format(Algorithm))\n",
    "        classifier= Models[Algorithm][0]\n",
    "        correct_benign=0\n",
    "        correct_malicious=0\n",
    "        convert_to_benign=0\n",
    "#         df=pd.read_csv(Algorithm+'_distillation.csv')\n",
    "#         X=df.iloc[:,:-1].values\n",
    "#         y=df.iloc[:,-1].values\n",
    "        target_model= Models[Algorithm][0]\n",
    "        test_dataset= Datasetcustom(X, y)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        for images, labels in test_loader:\n",
    "            images.requires_grad = True\n",
    "            model.zero_grad()\n",
    "            output = model(images)\n",
    "            y_pred = target_model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            data_grad = images.grad.data\n",
    "            if  y_pred[0][1]>y_pred[0][0] and labels[0]==1: #predicted malicious\n",
    "                grad_list=[]\n",
    "                for i in range(195):\n",
    "                    if images[0][i]==0:\n",
    "                        grad_list.append((data_grad[0][i],i))\n",
    "                grad_list.sort(key = lambda x: x[0], reverse=True)\n",
    "                flag=0\n",
    "                for i in range(0,10):\n",
    "                    images[0][grad_list[i][1]]=1\n",
    "                    y_pred = target_model(images)\n",
    "                    if y_pred[0][0]>y_pred[0][1]:\n",
    "                        for j in range(i,10):  #converted in i+1 flips, hence all bits>=i+1, the FN will increase\n",
    "                            df1.iloc[j+1,3]+=1\n",
    "                            df1.iloc[j+1,1]-=1\n",
    "                        flag=1\n",
    "                        convert_to_benign+=1\n",
    "                        break\n",
    "                if flag==0:          #Even after 10 flips, unable to convert to benignm update malicious counter\n",
    "                    correct_malicious+=1\n",
    "            elif y_pred[0][0]>y_pred[0][1] and labels[0]==1:  #If predicted as benign but is malicious\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,3]+=1\n",
    "                    df1.iloc[j,1]-=1\n",
    "                convert_to_benign+=1\n",
    "            elif  y_pred[0][0]>y_pred[0][1] and labels[0]==0:  #if predicted benign correctly\n",
    "                for j in range(0,11):\n",
    "                    df1.iloc[j,2]+=1\n",
    "                correct_benign+=1\n",
    "        \n",
    "        print(\"Correct benign is {} \\n correct malicious is {} \\n converted to benign is  {}\".format(correct_benign, correct_malicious, convert_to_benign))\n",
    "#         print(\"Final Accuracy is {:.3f} with max {} bits flipped\".format((correct_benign+5553-convert_benign)/11274, num_bits))\n",
    "        print(df1)\n",
    "        df1.to_csv(str(Algorithm)+\"_Distillation_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "mlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
